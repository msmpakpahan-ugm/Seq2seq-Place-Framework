{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Seq2seq dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noting the versions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version is incorrect: 1.23.0 (expected: 1.21.0)\n",
      "pandas version is incorrect: 1.4.3 (expected: 1.3.0)\n",
      "networkx version is incorrect: 2.8.4 (expected: 2.5.1)\n",
      "matplotlib version is incorrect: 3.5.2 (expected: 3.4.2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "\n",
    "def check_version(module, expected_version):\n",
    "    actual_version = module.__version__\n",
    "    if actual_version == expected_version:\n",
    "        print(f\"{module.__name__} version is correct: {actual_version}\")\n",
    "    else:\n",
    "        print(f\"{module.__name__} version is incorrect: {actual_version} (expected: {expected_version})\")\n",
    "\n",
    "check_version(np, \"1.21.0\")\n",
    "check_version(pd, \"1.3.0\")\n",
    "check_version(nx, \"2.5.1\")\n",
    "check_version(matplotlib, \"3.4.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def node_specification(data_topologi):\n",
    "    Speed =[]\n",
    "    ram =[]\n",
    "    storage=[]\n",
    "    id_node=[]\n",
    "    bandwith_node = []\n",
    "\n",
    "    for n in data_topologi['nodes']:\n",
    "        Speed.append(n['Speed'])\n",
    "        ram.append(n['RAM'])\n",
    "        storage.append(n['Storage'])\n",
    "        id_node.append(n['id'])\n",
    "        bandwith_node.append(75000)\n",
    "\n",
    "    df_nodes = pd.DataFrame(list(zip(id_node, Speed, ram, storage, bandwith_node)), columns =['id', 'Speed', 'RAM', 'Storage', 'Bandwith'])\n",
    "    return df_nodes\n",
    "\n",
    "def app_specification(data_app):\n",
    "    name_app_module=[]\n",
    "    ram_module=[]\n",
    "    storage_module= []\n",
    "    name_module=[]\n",
    "\n",
    "    name_app_message=[]\n",
    "    bytes_size=[]\n",
    "    instruction=[]\n",
    "    name_message=[]\n",
    "\n",
    "    deadline=[]\n",
    "\n",
    "    for k in data_app:\n",
    "        name_app = k['name']\n",
    "        for l in k['module']:\n",
    "            s = l['name']\n",
    "            namanya = s.replace(\"'\", \"\")\n",
    "            ram_module.append(l['RAM'])\n",
    "            storage_module.append(l['Storage'])\n",
    "            name_module.append(namanya)\n",
    "            name_app_module.append(name_app)\n",
    "            deadline.append(k['deadline'])\n",
    "\n",
    "        for m in k['message']:\n",
    "            name_app_message.append(name_app)\n",
    "            name_message.append(m['name'])\n",
    "            bytes_size.append(m['bytes'])\n",
    "            instruction.append(m['instructions'])\n",
    "\n",
    "    df_modules = pd.DataFrame(list(zip(name_app_module, name_module, ram_module, storage_module ,deadline)), columns =['app', 'Module', 'RAM', 'Storage', 'deadline'])\n",
    "    df_message = pd.DataFrame(list(zip(name_message, bytes_size, instruction)), columns =['Message', 'bytes', 'instruction'])\n",
    "    final_app = pd.concat([df_modules, df_message], axis=1)\n",
    "    return final_app\n",
    "\n",
    "def create_dict(topo):\n",
    "    new_dict = dict()\n",
    "    source = []\n",
    "    for i in topo['links']:\n",
    "        source.append(i['source'])\n",
    "        if i['source'] in new_dict:\n",
    "            # append the new number to the existing array at this slot\n",
    "            new_dict[i['source']].append(i['target'])\n",
    "            if i['target'] in new_dict:\n",
    "                new_dict[i['target']].append(i['source'])\n",
    "            else:\n",
    "                new_dict[i['target']] = [i['source']]\n",
    "        else:\n",
    "            # create a new array in this slot\n",
    "            new_dict[i['source']] = [i['target']]\n",
    "    return new_dict, source\n",
    "\n",
    "def checkthisout(place, appp, topo):\n",
    "    # nanti masukin pathnya aja\n",
    "    ## APP input\n",
    "\n",
    "\n",
    "    f = open(str(place))\n",
    "    data_placement = json.load(f)\n",
    "\n",
    "    df_nodes = topo\n",
    "\n",
    "\n",
    "    #framing placement\n",
    "    app =[]\n",
    "    Mod =[]\n",
    "    node = []\n",
    "\n",
    "    for n in data_placement['initialAllocation']:\n",
    "        app.append(n['app'])\n",
    "        Mod.append(n['module_name'])\n",
    "        node.append(n['id_resource'])\n",
    "    data_popo = pd.DataFrame(list(zip(app, Mod, node)), columns =['App', 'Mod', 'node'])\n",
    "\n",
    "    df_apps = appp\n",
    "    muatan = []\n",
    "    for index, row in data_popo.iterrows():\n",
    "        current_mod = df_apps.loc[(df_apps['app'] == str(row['App']))&(df_apps['Module'] == str(row['Mod']))]\n",
    "        df_nodes.loc[row['node'], 'Storage'] = int(df_nodes.loc[row['node'], 'Storage']) - int(current_mod['Storage'])\n",
    "        # df_nodes.loc[row['node'], 'IPT'] = int(df_nodes.loc[row['node'], 'IPT']) - int(current_mod['Instruction'])\n",
    "\n",
    "        # df_nodes.loc[row['node'], 'Bandwith'] = int(df_nodes.loc[row['node'], 'Bandwith']) - int(current_mod['Bandwith'])\n",
    "\n",
    "        # if((df_nodes.loc[row['node'], 'IPT'] <= 0) or (df_nodes.loc[row['node'], 'RAM'] <= 0) or (df_nodes.loc[row['node'], 'Bandwith'] <= 0)):\n",
    "        #     muatan.append('Tidak Muat')\n",
    "        # else:\n",
    "        #     muatan.append('Muat')\n",
    "\n",
    "        if(df_nodes.loc[row['node'], 'RAM'] <= 0):\n",
    "            muatan.append('Tidak Muat')\n",
    "        else:\n",
    "            muatan.append('Muat')\n",
    "    return muatan\n",
    "\n",
    "def createlistofplacement(linkplacement):\n",
    "    f = open(str(linkplacement))\n",
    "    data_placement = json.load(f)\n",
    "\n",
    "    #framing placement\n",
    "    app =[]\n",
    "    Mod =[]\n",
    "    node = []\n",
    "\n",
    "    for n in data_placement['initialAllocation']:\n",
    "        app.append(n['app'])\n",
    "        Mod.append(n['module_name'])\n",
    "        node.append(n['id_resource'])\n",
    "    data_popo = pd.DataFrame(list(zip(app, Mod, node)), columns =['App', 'Mod', 'node'])\n",
    "    return data_popo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_extract_to_dataframe(filepath):\n",
    "\n",
    "    try:\n",
    "        # Reading and parsing the JSON file\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Extracting the 'nodes' data\n",
    "        nodes_data = data['nodes']\n",
    "\n",
    "        # Creating a DataFrame from the nodes data\n",
    "        return pd.DataFrame(nodes_data)\n",
    "    except Exception as e:\n",
    "        # In case of any error, return the error message\n",
    "        return str(e)\n",
    "\n",
    "def module_json_to_dataframe(json_file_path):\n",
    "    # Read the JSON file\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Prepare an empty list to store rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate over each application\n",
    "    for app in data:\n",
    "        app_id = app['id']\n",
    "        numberofmodule = app['numberofmodule']\n",
    "\n",
    "        # Iterate over each module and message\n",
    "        for module, message in zip(app['module'], app['message']):\n",
    "            row = {\n",
    "                'Application ID': str(app_id),\n",
    "                'numberofmodule': numberofmodule,\n",
    "                'Module ID': module['id'],\n",
    "                'Module Name': module['name'],\n",
    "                'Required RAM': module['RAM'],\n",
    "                'Required Storage': module['Storage'],\n",
    "                'Message ID': message['id'],\n",
    "                'Message Name': message['name'],\n",
    "                'Bytes': message['bytes'],\n",
    "                'Instructions': message['instructions']\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "    # Convert the list of rows into a DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df\n",
    "\n",
    "def placement_json_to_dataframe(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    rows = []\n",
    "    for placement in data['initialAllocation']:\n",
    "        rows.append({\n",
    "            'Module Name': str(placement['module_name']),\n",
    "            'Application ID': str(placement['app']),\n",
    "            'Node ID': placement['id_resource']\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Main function to check placement validity\n",
    "def check_placement_validity(topology_file, application_file, placement_file):\n",
    "    # Parse the JSON files into DataFrames\n",
    "    nodes_df = nodes_extract_to_dataframe(topology_file)\n",
    "    application_df = module_json_to_dataframe(application_file)\n",
    "    placement_df = placement_json_to_dataframe(placement_file)\n",
    "\n",
    "\n",
    "    # # # Diagnostic prints to check DataFrame structures\n",
    "    # print(\"Placement DataFrame Columns:\", placement_df.columns)\n",
    "    # print(\"Application DataFrame Columns:\", application_df.columns)\n",
    "    # print(\"Data types in Placement DataFrame:\", placement_df.dtypes)\n",
    "    # print(\"Data types in Application DataFrame:\", application_df.dtypes)\n",
    "\n",
    "    # Merge and calculate Storage usage\n",
    "    placement_app_merged = pd.merge(placement_df, application_df, on=['Module Name', 'Application ID'], how='left')\n",
    "    node_ram_usage = placement_app_merged.groupby('Node ID')['Required Storage'].sum().reset_index()\n",
    "    nodes_df.rename(columns={'Storage': 'Available Storage'}, inplace=True)\n",
    "\n",
    "    # Compare available and required Storage\n",
    "    node_Storage_comparison = pd.merge(nodes_df, node_ram_usage, left_on='id', right_on='Node ID', how='left')\n",
    "    node_Storage_comparison['Required Storage'].fillna(0, inplace=True)\n",
    "    node_Storage_comparison['Storage_Sufficient'] = node_ram_comparison['Available Storage'] >= node_ram_comparison['Required Storage']\n",
    "\n",
    "       # Check if all nodes have sufficient Storage and print a message if so\n",
    "    if node_Storage_comparison['Storage_Sufficient'].all():\n",
    "        print(\"All is good.\")\n",
    "        return node_Storage_comparison[['id', 'Available Storage', 'Required Storage', 'Storage_Sufficient']]\n",
    "    else:\n",
    "        print(\"Something not fit\")\n",
    "        return node_Storage_comparison[['id', 'Available Storage', 'Required Storage', 'Storage_Sufficient']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HopAware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Helper function to safely get the next node choice\n",
    "def get_next_pilihan(current_pilihan, new_dict):\n",
    "    \"\"\"Safely gets a random neighbor or defaults to cloud (100).\"\"\"\n",
    "    try:\n",
    "        # Ensure key is of the correct type for new_dict (assuming int)\n",
    "        current_pilihan_key = int(current_pilihan)\n",
    "\n",
    "        if current_pilihan_key == 100:\n",
    "            return 100 # Stay in cloud if already there\n",
    "\n",
    "        neighbors = new_dict.get(current_pilihan_key, [])\n",
    "        if neighbors:\n",
    "            # Ensure neighbors are suitable types before returning\n",
    "            chosen_neighbor = random.choice(neighbors)\n",
    "            return int(chosen_neighbor) # Assuming neighbors are node IDs\n",
    "        else:\n",
    "            # No neighbors found for the current node, default to cloud\n",
    "            # print(f\"Warning: Node {current_pilihan_key} has no neighbors in new_dict. Defaulting to cloud.\")\n",
    "            return 100\n",
    "    except (ValueError, TypeError) as e:\n",
    "        # print(f\"Warning: Error processing pilihan {current_pilihan} or its neighbors: {e}. Defaulting to cloud.\")\n",
    "        return 100\n",
    "    except Exception as e: # Catch other potential errors\n",
    "        # print(f\"Unexpected error getting next pilihan for {current_pilihan}: {e}. Defaulting to cloud.\")\n",
    "        return 100\n",
    "\n",
    "# Renaming the function to match the user's selection\n",
    "def create_placementold_newformat(data_pop, new_dict, app_spec, node_spec, popul, app, save_folder, hopnumber):\n",
    "    JSONfile = {\"initialAllocation\": []}\n",
    "    allocation = [] # List to store allocation dicts for JSON\n",
    "    output_lines = [] # List to store formatted strings for TXT\n",
    "    order = 0  # Initialize order counter for TXT output\n",
    "\n",
    "    # Create a local copy of node Storage to modify during placement simulation for this run.\n",
    "    # This prevents modifications affecting subsequent calls or applications if node_spec is reused.\n",
    "    # Ensure index is usable (assuming 'id' column was used or index is already int)\n",
    "    try:\n",
    "        if not pd.api.types.is_integer_dtype(node_spec.index):\n",
    "             # If index isn't integer, try using 'id' column if it exists and is suitable\n",
    "             if 'id' in node_spec.columns and pd.api.types.is_numeric_dtype(node_spec['id']):\n",
    "                 node_spec = node_spec.set_index('id', drop=False) # Keep id column if needed elsewhere\n",
    "             else:\n",
    "                 # Fallback or raise error if no suitable integer index can be found\n",
    "                 print(\"Warning: node_spec index is not integer, Storage lookup might fail.\")\n",
    "        # Use 'Storage' instead of 'RAM'\n",
    "        local_node_spec_storage = node_spec['Storage'].astype(int).copy()\n",
    "    except Exception as e:\n",
    "        # Use 'Storage' instead of 'RAM'\n",
    "        print(f\"Error preparing local node Storage spec: {e}. Aborting placement.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "    for i in data_pop['sources']:\n",
    "        source_node_id = i['id_resource']\n",
    "        applik = i['app'] # Application identifier\n",
    "        # Get lambda value, default to 'N/A' if not found\n",
    "        lambda_val = i.get('lambda', 'N/A')\n",
    "\n",
    "        # --- Determine initial starting node ('pilihan') based on hopnumber ---\n",
    "        try:\n",
    "            source_node_int = int(source_node_id)\n",
    "            if hopnumber == 3:\n",
    "                pilihan = get_next_pilihan(source_node_int, new_dict) # Start from a neighbor\n",
    "                # print(f\"HOP3: Initial pilihan {pilihan} (neighbor of {source_node_int})\")\n",
    "            elif hopnumber == 2:\n",
    "                pilihan = source_node_int # Start from the source node itself\n",
    "                # print(f\"HOP2: Initial pilihan {pilihan} (source node)\")\n",
    "            else:\n",
    "                # print(f\"Warning: Unsupported hopnumber {hopnumber}. Defaulting to HOP2 logic.\")\n",
    "                pilihan = source_node_int\n",
    "        except (ValueError, TypeError) as e:\n",
    "            # print(f\"Warning: Invalid source node ID {source_node_id}: {e}. Starting at cloud.\")\n",
    "            pilihan = 100 # Default to cloud if source ID is invalid\n",
    "        except Exception as e:\n",
    "             # print(f\"Unexpected error determining initial pilihan for source {source_node_id}: {e}. Starting at cloud.\")\n",
    "             pilihan = 100\n",
    "\n",
    "        # --- Filter application specification ---\n",
    "        try:\n",
    "            # Ensure comparison works (e.g., both strings or both ints)\n",
    "            # Assuming app_spec['app'] might be int/str, applik is likely str from JSON\n",
    "            current_app = app_spec[app_spec['app'].astype(str) == str(applik)]\n",
    "        except Exception as e:\n",
    "            print(f\"Error filtering app_spec for app {applik}: {e}. Skipping application.\")\n",
    "            continue\n",
    "\n",
    "        if current_app.empty:\n",
    "             # print(f\"Warning: No modules found for application {applik} in app_spec. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        # --- Initialize for TXT output for this application ---\n",
    "        module_count = len(current_app)\n",
    "        placement_line = [] # Stores node placements for TXT format (e.g., \"n5\", \"n100\")\n",
    "        # Initialize spec_line with order, module count, and lambda\n",
    "        spec_line = [f\"o{order}\", f\"m{module_count}\", f\"l{lambda_val}\"] # Stores spec info for TXT format\n",
    "        failed_nodes = []  # Stores failed nodes for TXT format (e.g., \"f3\")\n",
    "\n",
    "        # --- Iterate through modules of the current application ---\n",
    "        for index, row in current_app.iterrows():\n",
    "            module_name = str(row.get('Module', f'UnknownModule_{index}')) # Safely get module name\n",
    "            try:\n",
    "                # Use 'Storage' instead of 'RAM'\n",
    "                required_storage = int(row['Storage'])\n",
    "            except (KeyError, ValueError, TypeError) as e:\n",
    "                # Use 'Storage' instead of 'RAM'\n",
    "                print(f\"Warning: Invalid Storage for module {module_name} (App {applik}): {row.get('Storage', 'N/A')}. Skipping module. Error: {e}\")\n",
    "                placement_line.append(\"n?_invalid_storage\") # Indicate skip in TXT\n",
    "                spec_line.append(f\"r?_invalid\") # Indicate skip in TXT spec (keep 'r' prefix)\n",
    "                continue # Skip this module\n",
    "\n",
    "            # Use 'Storage' instead of 'RAM'\n",
    "            spec_line.append(f\"r{required_storage}\")  # Add required Storage to TXT spec line\n",
    "\n",
    "            ketemu = False # Flag: Found placement for this module?\n",
    "            count_first = 0 # Outer loop counter (placement attempt sequences)\n",
    "\n",
    "            # Store the node pilihan *before* starting the search for this module\n",
    "            current_pilihan_state = pilihan # This holds the node to try *next*\n",
    "\n",
    "            while count_first < 3 and not ketemu:\n",
    "                count = 0 # Inner loop counter (hops within a sequence)\n",
    "                node_to_evaluate = current_pilihan_state # Node to check in this hop\n",
    "\n",
    "                while count < 5: # Try up to 5 hops\n",
    "                    string1 = {} # To store JSON allocation info for this module\n",
    "\n",
    "                    try:\n",
    "                        node_to_evaluate_int = int(node_to_evaluate)\n",
    "                    except (ValueError, TypeError):\n",
    "                        # print(f\"Warning: Invalid node ID {node_to_evaluate} during search. Treating as cloud.\")\n",
    "                        node_to_evaluate_int = 100\n",
    "\n",
    "                    # --- Check Cloud ---\n",
    "                    if node_to_evaluate_int == 100:\n",
    "                        # print(f\"Placing module {module_name} (App {applik}) on Cloud (100).\")\n",
    "                        string1 = {\"module_name\": module_name, \"app\": str(applik), \"id_resource\": 100}\n",
    "                        placement_line.append(\"n100\")\n",
    "                        current_pilihan_state = 100 # Next module search also starts at cloud\n",
    "                        ketemu = True\n",
    "                        break # Exit inner loop (count < 5)\n",
    "\n",
    "                    # --- Check Node Resources ---\n",
    "                    try:\n",
    "                        # Use 'Storage' instead of 'RAM'\n",
    "                        available_storage = int(local_node_spec_storage.loc[node_to_evaluate_int])\n",
    "                        # Use 'Storage' instead of 'RAM'\n",
    "                        storage_after_placement = available_storage - required_storage\n",
    "\n",
    "                        # Use 'Storage' instead of 'RAM'\n",
    "                        if storage_after_placement >= 0:\n",
    "                            # Placement success on this node\n",
    "                            # Use 'Storage' instead of 'RAM'\n",
    "                            # print(f\"Placing module {module_name} (App {applik}) on Node {node_to_evaluate_int} (Storage {available_storage} -> {storage_after_placement}).\")\n",
    "                            string1 = {\"module_name\": module_name, \"app\": str(applik), \"id_resource\": node_to_evaluate_int}\n",
    "                            placement_line.append(f\"n{node_to_evaluate_int}\")\n",
    "\n",
    "                            # Update the local Storage copy\n",
    "                            # Use 'Storage' instead of 'RAM'\n",
    "                            local_node_spec_storage.loc[node_to_evaluate_int] = storage_after_placement\n",
    "\n",
    "                            # Determine starting node for the *next* module's search\n",
    "                            current_pilihan_state = get_next_pilihan(node_to_evaluate_int, new_dict)\n",
    "                            # print(f\"Next module search will start near node {current_pilihan_state}\")\n",
    "\n",
    "                            ketemu = True\n",
    "                            break # Exit inner loop (count < 5)\n",
    "                        else:\n",
    "                            # Insufficient Storage on this node\n",
    "                            # Use 'Storage' instead of 'RAM'\n",
    "                            # print(f\"Node {node_to_evaluate_int} insufficient Storage ({available_storage} < {required_storage}) for {module_name}. Trying next hop.\")\n",
    "                            failed_node_str = f\"f{node_to_evaluate_int}\"\n",
    "                            if failed_node_str not in failed_nodes: # Avoid duplicates in TXT\n",
    "                                failed_nodes.append(failed_node_str)\n",
    "\n",
    "                            # Move to the next node in the hop sequence\n",
    "                            node_to_evaluate = get_next_pilihan(node_to_evaluate_int, new_dict)\n",
    "                            # print(f\"Hopping to node {node_to_evaluate}\")\n",
    "\n",
    "                    except (KeyError):\n",
    "                        # Use 'Storage' instead of 'RAM'\n",
    "                        # print(f\"Warning: Node {node_to_evaluate_int} not found in local Storage spec. Treating as failed.\")\n",
    "                        failed_node_str = f\"f{node_to_evaluate_int}_not_found\"\n",
    "                        if failed_node_str not in failed_nodes: failed_nodes.append(failed_node_str)\n",
    "                        node_to_evaluate = get_next_pilihan(node_to_evaluate_int, new_dict) # Try neighbor\n",
    "                    except (ValueError, TypeError) as e:\n",
    "                         # Use 'Storage' instead of 'RAM'\n",
    "                         print(f\"Warning: Error processing Storage for node {node_to_evaluate_int}: {e}. Treating as failed.\")\n",
    "                         # Use 'Storage' instead of 'RAM'\n",
    "                         failed_node_str = f\"f{node_to_evaluate_int}_storage_error\"\n",
    "                         if failed_node_str not in failed_nodes: failed_nodes.append(failed_node_str)\n",
    "                         node_to_evaluate = get_next_pilihan(node_to_evaluate_int, new_dict) # Try neighbor\n",
    "\n",
    "                    count += 1 # Increment inner loop counter (hops)\n",
    "                # --- End of inner loop (while count < 5) ---\n",
    "\n",
    "                if ketemu:\n",
    "                    break # Exit outer loop (count_first < 3)\n",
    "                else:\n",
    "                    # Failed to find placement within 5 hops starting from where we left off.\n",
    "                    # The example logic seems to just continue from the last 'pilihan' state.\n",
    "                    # The outer loop (count_first) in the example seems more like a safeguard against infinite loops\n",
    "                    # rather than restarting the search from different points. Let's follow that.\n",
    "                    # If the inner loop finished, current_pilihan_state holds the last node tried or hopped to.\n",
    "                    # We just continue the outer loop check. If count_first reaches 3, we fail.\n",
    "                    # print(f\"Placement attempt sequence {count_first + 1} failed for module {module_name}.\")\n",
    "                    pass # Just let the outer loop increment\n",
    "\n",
    "                count_first += 1 # Increment outer loop counter\n",
    "            # --- End of outer loop (while count_first < 3) ---\n",
    "\n",
    "            # --- Handle placement failure after all attempts ---\n",
    "            if not ketemu:\n",
    "                # print(f\"Failed to place module {module_name} (App {applik}) after {count_first} attempts. Placing on Cloud (100).\")\n",
    "                string1 = {\"module_name\": module_name, \"app\": str(applik), \"id_resource\": 100}\n",
    "                placement_line.append(\"n100\")\n",
    "                current_pilihan_state = 100 # Ensure next module search starts at cloud\n",
    "\n",
    "            # --- Add allocation result to JSON list ---\n",
    "            if string1: # Only append if placement was determined\n",
    "                allocation.append(string1)\n",
    "            else:\n",
    "                 # This case should ideally not happen if failure defaults to cloud\n",
    "                 print(f\"Error: Module {module_name} (App {applik}) finished without allocation decision.\")\n",
    "                 placement_line.append(\"n?_error\") # Indicate error in TXT\n",
    "\n",
    "            # Update the main 'pilihan' state for the next module based on where this one ended up\n",
    "            pilihan = current_pilihan_state\n",
    "\n",
    "        # --- Finalize TXT output lines for this application ---\n",
    "        spec_line.append(f\"n{source_node_id}\") # Add source node ID\n",
    "        spec_line.extend(failed_nodes) # Add recorded failed nodes\n",
    "        output_lines.append(\" \".join(spec_line))\n",
    "        output_lines.append(\" \".join(placement_line))\n",
    "\n",
    "        order += 1  # Increment order for the next application\n",
    "\n",
    "    # --- End of application loop (for i in data_pop['sources']) ---\n",
    "\n",
    "    # --- Save JSON output ---\n",
    "    JSONfile[\"initialAllocation\"] = allocation\n",
    "    json_object = json.dumps(JSONfile, indent=2)\n",
    "    json_filename = f\"placement\\D100_P{popul}_A{app}_hop{hopnumber}.json\"\n",
    "    json_filepath = os.path.join(save_folder, json_filename)\n",
    "    try:\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        with open(json_filepath, \"w\") as outfile: # Write mode for JSON\n",
    "            outfile.write(json_object)\n",
    "        # print(f\"JSON allocation saved to {json_filepath}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing JSON file {json_filepath}: {e}\")\n",
    "        # Consider returning False or raising exception\n",
    "\n",
    "    # --- Save TXT output ---\n",
    "    txt_filename = f\"D100_P{popul}_A{app}_hop{hopnumber}.txt\"\n",
    "    txt_filepath = os.path.join(save_folder, txt_filename)\n",
    "    try:\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        # Use append mode 'a' for the TXT file\n",
    "        with open(txt_filepath, \"a\") as outfile:\n",
    "            # Add a newline before appending if the file already exists and is not empty\n",
    "            if os.path.exists(txt_filepath) and os.path.getsize(txt_filepath) > 0:\n",
    "                 outfile.write(\"\\n\")\n",
    "            outfile.write(\"\\n\".join(output_lines)) # Write the lines for the current run\n",
    "        # print(f\"TXT decision data appended to {txt_filepath}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error appending TXT file {txt_filepath}: {e}\")\n",
    "        # Consider returning False or raising exception\n",
    "\n",
    "    return True # Indicate success (or partial success if errors occurred but were handled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "execution_times = []\n",
    "methodname = \"hop3_withfailure\"\n",
    "hopnumba = 2\n",
    "\n",
    "pop = 0\n",
    "app = 10\n",
    "\n",
    "f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\topology\\100m2.json')\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "# print(f)\n",
    "\n",
    "f = open(fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\population\\100m2_P{pop}_E{app}.json\")\n",
    "data_pop = json.load(f)\n",
    "# print(f)\n",
    "\n",
    "f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\application\\E{app}_{pop}.json')\n",
    "data_app = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "new_dict, source = create_dict(data)\n",
    "app_spec = app_specification(data_app)\n",
    "node_spec = node_specification(data)\n",
    "source = list(dict.fromkeys(source))\n",
    "pop_id = pop\n",
    "app_id = app\n",
    "\n",
    "save_folder = fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\training_data\"\n",
    "# Time the execution of create_placementold\n",
    "start_time = time.time()\n",
    "create_placementold_newformat(data_pop, new_dict, app_spec, node_spec, pop_id, app_id, save_folder, hopnumber=hopnumba)\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "execution_times.append({'pop': pop, 'app': app, 'execution_time': execution_time, 'methodname': methodname})\n",
    "\n",
    "# Create DataFrame from execution times\n",
    "df_execution_times = pd.DataFrame(execution_times)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_execution_times.to_csv(f\"execution_times_{methodname}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app</th>\n",
       "      <th>Module</th>\n",
       "      <th>RAM</th>\n",
       "      <th>Storage</th>\n",
       "      <th>deadline</th>\n",
       "      <th>Message</th>\n",
       "      <th>bytes</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mod0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>26658</td>\n",
       "      <td>M.USER.APP.0</td>\n",
       "      <td>1652773</td>\n",
       "      <td>59402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Mod1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>26658</td>\n",
       "      <td>M1</td>\n",
       "      <td>3046134</td>\n",
       "      <td>46009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Mod0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6664</td>\n",
       "      <td>M.USER.APP.1</td>\n",
       "      <td>4356231</td>\n",
       "      <td>29732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Mod1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6664</td>\n",
       "      <td>M1</td>\n",
       "      <td>1626814</td>\n",
       "      <td>26718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Mod2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6664</td>\n",
       "      <td>M2</td>\n",
       "      <td>2194439</td>\n",
       "      <td>21605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>99</td>\n",
       "      <td>Mod5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13116</td>\n",
       "      <td>M5</td>\n",
       "      <td>2926084</td>\n",
       "      <td>55616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>99</td>\n",
       "      <td>Mod6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13116</td>\n",
       "      <td>M6</td>\n",
       "      <td>4266991</td>\n",
       "      <td>56826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>99</td>\n",
       "      <td>Mod7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>13116</td>\n",
       "      <td>M7</td>\n",
       "      <td>4234856</td>\n",
       "      <td>20269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>99</td>\n",
       "      <td>Mod8</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>13116</td>\n",
       "      <td>M8</td>\n",
       "      <td>2473891</td>\n",
       "      <td>36021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>99</td>\n",
       "      <td>Mod9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13116</td>\n",
       "      <td>M9</td>\n",
       "      <td>3566881</td>\n",
       "      <td>43607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>579 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    app Module  RAM  Storage  deadline       Message    bytes  instruction\n",
       "0     0   Mod0    6        4     26658  M.USER.APP.0  1652773        59402\n",
       "1     0   Mod1    1        4     26658            M1  3046134        46009\n",
       "2     1   Mod0    3        6      6664  M.USER.APP.1  4356231        29732\n",
       "3     1   Mod1    4        3      6664            M1  1626814        26718\n",
       "4     1   Mod2    1        5      6664            M2  2194439        21605\n",
       "..   ..    ...  ...      ...       ...           ...      ...          ...\n",
       "574  99   Mod5    3        2     13116            M5  2926084        55616\n",
       "575  99   Mod6    2        2     13116            M6  4266991        56826\n",
       "576  99   Mod7    5        6     13116            M7  4234856        20269\n",
       "577  99   Mod8    6        5     13116            M8  2473891        36021\n",
       "578  99   Mod9    2        2     13116            M9  3566881        43607\n",
       "\n",
       "[579 rows x 8 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hop3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "execution_times = []\n",
    "methodname = \"hop3_withfailure\"\n",
    "\n",
    "for pop in range(10):\n",
    "    for app in range(10):\n",
    "\n",
    "        f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\topology\\100m2.json')\n",
    "        # a dictionary\n",
    "        data = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        f = open(fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\population\\100m2_P{pop}_D100.json\")\n",
    "        # a dictionary\n",
    "        data_pop = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\application\\D100_{app}.json')\n",
    "        data_app = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        new_dict, source = create_dict(data)\n",
    "        app_spec = app_specification(data_app)\n",
    "        node_spec = node_specification(data)\n",
    "        source = list(dict.fromkeys(source))\n",
    "        pop_id = pop\n",
    "        app_id = app\n",
    "\n",
    "        save_folder = fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\training_data\"\n",
    "        # Time the execution of create_placementold\n",
    "        start_time = time.time()\n",
    "        create_placementold_newformat(data_pop, new_dict, app_spec, node_spec, pop_id, app_id, save_folder, hopnumber=3)\n",
    "        end_time = time.time()\n",
    "\n",
    "        execution_time = end_time - start_time\n",
    "        execution_times.append({'pop': pop, 'app': app, 'execution_time': execution_time, 'methodname': methodname})\n",
    "\n",
    "# Create DataFrame from execution times\n",
    "df_execution_times = pd.DataFrame(execution_times)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_execution_times.to_csv(f\"execution_times_{methodname}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hop2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "execution_times = []\n",
    "methodname = \"hop2_withfailure\"\n",
    "\n",
    "for pop in range(10):\n",
    "    for app in range(10):\n",
    "\n",
    "        f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\topology\\100m2.json')\n",
    "        # a dictionary\n",
    "        data = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        f = open(fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\population\\100m2_P{pop}_D100.json\")\n",
    "        # a dictionary\n",
    "        data_pop = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\application\\D100_{app}.json')\n",
    "        data_app = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        new_dict, source = create_dict(data)\n",
    "        app_spec = app_specification(data_app)\n",
    "        node_spec = node_specification(data)\n",
    "        source = list(dict.fromkeys(source))\n",
    "        pop_id = pop\n",
    "        app_id = app\n",
    "\n",
    "        save_folder = fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\training_data\"\n",
    "        # Time the execution of create_placementold\n",
    "        start_time = time.time()\n",
    "        create_placementold_newformat(data_pop, new_dict, app_spec, node_spec, pop_id, app_id, save_folder, hopnumber=2)\n",
    "        end_time = time.time()\n",
    "\n",
    "        execution_time = end_time - start_time\n",
    "        execution_times.append({'pop': pop, 'app': app, 'execution_time': execution_time, 'methodname': methodname})\n",
    "\n",
    "# Create DataFrame from execution times\n",
    "df_execution_times = pd.DataFrame(execution_times)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_execution_times.to_csv(f\"execution_times_{methodname}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# firstfit_hopware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Modified find_suitable_node to return failed attempts for logging\n",
    "# Replaced 'RAM' with 'Storage'\n",
    "def find_suitable_node(row, node_spec, current_resource_id, adjacency_dict, cloud_node_id):\n",
    "    \"\"\"\n",
    "    Find a suitable node for module placement based on Storage with a fallback to cloud node.\n",
    "    Now also returns a list of nodes that failed due to insufficient Storage during this search.\n",
    "    \"\"\"\n",
    "    failed_nodes_for_this_call = [] # Track nodes that failed in this specific search\n",
    "    original_start_node = current_resource_id # Keep track of where this search started\n",
    "\n",
    "    for retry in range(5):  # retry up to 5 times\n",
    "        search_path_this_retry = [] # Track path within a retry to avoid adding duplicates quickly\n",
    "        temp_resource_id = original_start_node if retry == 0 else current_resource_id # Start fresh or continue from last random hop\n",
    "\n",
    "        for attempt in range(5):  # try up to 5 times per retry\n",
    "            available_storage = -1 # Default to -1 if node ID is invalid or not found\n",
    "            node_to_check = int(temp_resource_id) # Ensure integer ID\n",
    "\n",
    "            if node_to_check in node_spec.index:\n",
    "                 # Use .loc for safe access using 'Storage' column\n",
    "                available_storage = node_spec.loc[node_to_check, 'Storage'] - row['Storage']\n",
    "                # print(f\"Retry {retry + 1}, attempt {attempt + 1} for module {row['Module']} on node {node_to_check}.\")\n",
    "                # print(f\"Node {node_to_check} has {node_spec.loc[node_to_check, 'Storage']} Storage, app needed {row['Storage']}, available Storage after placement {available_storage}\")\n",
    "            else:\n",
    "                # print(f\"Warning: Node {node_to_check} not found in node_spec index during search.\")\n",
    "                # Treat as failed node? Or rely on random choice moving away?\n",
    "                # Let's log it as failed and try moving away.\n",
    "                 if node_to_check not in failed_nodes_for_this_call:\n",
    "                      failed_nodes_for_this_call.append(node_to_check)\n",
    "                 # Try to pick a neighbor of the *previous valid node* if possible, or fall back\n",
    "                 # This part of the original logic might need refinement if node IDs can be invalid often\n",
    "                 # For now, let's try to move from the last known *good* or attempted node in the path if possible\n",
    "                 last_known_node = search_path_this_retry[-1] if search_path_this_retry else original_start_node\n",
    "                 try:\n",
    "                     temp_resource_id = random.choice(adjacency_dict.get(int(last_known_node), [cloud_node_id])) # Default to cloud if no neighbors\n",
    "                 except (ValueError, TypeError):\n",
    "                     temp_resource_id = cloud_node_id # Fallback if ID conversion fails\n",
    "                 continue # Skip to next attempt\n",
    "\n",
    "            if available_storage >= 0:\n",
    "                # print(\"Suitable placement found.\")\n",
    "                # Return chosen node, remaining Storage, and the list of nodes failed *during this search*\n",
    "                return node_to_check, available_storage, failed_nodes_for_this_call\n",
    "\n",
    "            # If here, placement failed on node_to_check due to insufficient Storage\n",
    "            if node_to_check not in failed_nodes_for_this_call:\n",
    "                 failed_nodes_for_this_call.append(node_to_check)\n",
    "            search_path_this_retry.append(node_to_check)\n",
    "\n",
    "            # Move to a random neighbor for the next attempt (original logic)\n",
    "            try:\n",
    "                # Ensure neighbors exist and handle potential errors\n",
    "                neighbors = adjacency_dict.get(node_to_check, [])\n",
    "                if not neighbors:\n",
    "                    # print(f\"Node {node_to_check} has no neighbors. Jumping to cloud for next attempt.\")\n",
    "                    temp_resource_id = cloud_node_id # Jump to cloud if no neighbors\n",
    "                else:\n",
    "                    temp_resource_id = random.choice(neighbors)\n",
    "                # print(f\"Not enough Storage on {node_to_check}. Trying a new node: {temp_resource_id}\")\n",
    "            except (ValueError, TypeError, KeyError) as e:\n",
    "                 # print(f\"Error finding neighbors for {node_to_check}: {e}. Jumping to cloud.\")\n",
    "                 temp_resource_id = cloud_node_id # Fallback to cloud\n",
    "\n",
    "        # After 5 attempts, select a new resource_id randomly for the next retry (original logic)\n",
    "        # Use the last attempted node (temp_resource_id) as the base for the next random hop\n",
    "        try:\n",
    "            neighbors = adjacency_dict.get(int(temp_resource_id), [])\n",
    "            if not neighbors:\n",
    "                # print(f\"Node {temp_resource_id} (end of retry {retry+1}) has no neighbors. Retrying from cloud.\")\n",
    "                current_resource_id = cloud_node_id\n",
    "            else:\n",
    "                current_resource_id = random.choice(neighbors)\n",
    "            # print(f\"Retrying with a new node: {current_resource_id} after 5 failed attempts in retry {retry + 1}.\")\n",
    "        except (ValueError, TypeError, KeyError) as e:\n",
    "            # print(f\"Error finding neighbors for {temp_resource_id} before retry: {e}. Retrying from cloud.\")\n",
    "            current_resource_id = cloud_node_id # Fallback to cloud\n",
    "\n",
    "    # If suitable placement is not found after 5 retries, place on cloud node (original logic)\n",
    "    # print(\"Failed to find a suitable edge node after 5 retries. Placing on cloud node.\")\n",
    "    # Return cloud node, its *original* Storage (calling function handles subtraction), and collected failed nodes\n",
    "    return cloud_node_id, node_spec.loc[cloud_node_id, 'Storage'], failed_nodes_for_this_call\n",
    "\n",
    "# Modified process_application to handle logging data collection and return TXT lines\n",
    "# Replaced 'RAM' with 'Storage'\n",
    "def process_application(app_specifications, source, node_spec, adjacency_dict, cloud_node_id):\n",
    "    \"\"\"\n",
    "    Process each application for placement based on Storage with cloud node fallback.\n",
    "    Now collects data for TXT logging and returns formatted TXT lines.\n",
    "    \"\"\"\n",
    "    allocation = [] # For JSON output\n",
    "    placement_line_nodes = [] # For TXT: stores n{id} placements\n",
    "    required_storage_list = [] # For TXT: stores r{storage} requirements\n",
    "    all_failed_nodes_for_app = set() # For TXT: stores unique f{id} failed nodes for this app\n",
    "\n",
    "    # Get required info from source for TXT header\n",
    "    app_name = source['app']\n",
    "    source_node_id = source['id_resource']\n",
    "    lambda_val = source.get('lambda', 'N/A') # Safely get lambda\n",
    "    module_count = len(app_specifications)\n",
    "\n",
    "    # print(f\"Processing application: {app_name} from source {source_node_id}\")\n",
    "    # Initial resource_id for the *first* module search comes from the source\n",
    "    current_search_start_node = source_node_id\n",
    "\n",
    "    for _, row in app_specifications.iterrows():\n",
    "        module_name = row['Module']\n",
    "        required_storage = int(row['Storage']) # Assume Storage is integer\n",
    "        required_storage_list.append(f\"r{required_storage}\") # Store for TXT spec line\n",
    "\n",
    "        # Call modified find_suitable_node\n",
    "        # Pass the node where the *previous* module was placed (or source node initially)\n",
    "        # as the starting point for the search\n",
    "        resource_id, available_storage_after_placement, failed_nodes_for_this_call = find_suitable_node(\n",
    "            row, node_spec, current_search_start_node, adjacency_dict, cloud_node_id\n",
    "        )\n",
    "\n",
    "        # Update the set of all failed nodes encountered for this application\n",
    "        all_failed_nodes_for_app.update(failed_nodes_for_this_call)\n",
    "\n",
    "        # Update node_spec Storage (original logic adapted)\n",
    "        # If placed on cloud, available_storage_after_placement is total cloud Storage, so subtract now\n",
    "        if resource_id == cloud_node_id:\n",
    "            # print(f\"Module {module_name} placed on Cloud ({cloud_node_id}). Updating cloud Storage.\")\n",
    "            node_spec.loc[resource_id, 'Storage'] -= required_storage\n",
    "        else:\n",
    "            # print(f\"Module {module_name} placed on Node {resource_id}. Updating node Storage.\")\n",
    "            node_spec.loc[resource_id, 'Storage'] = available_storage_after_placement # This was already calculated correctly\n",
    "\n",
    "        # Record placement for JSON\n",
    "        placement = {\"module_name\": module_name, \"app\": app_name, \"id_resource\": resource_id}\n",
    "        allocation.append(placement)\n",
    "\n",
    "        # Record placement for TXT\n",
    "        placement_line_nodes.append(f\"n{resource_id}\")\n",
    "        # print(f\"Module {module_name} placed on node {resource_id}. Available Storage left: {node_spec.loc[resource_id, 'Storage']}\")\n",
    "\n",
    "        # The node where this module was placed becomes the starting point for the next module's search\n",
    "        current_search_start_node = resource_id\n",
    "\n",
    "    # --- Format TXT output lines for this application ---\n",
    "    # Line 1: o{order} m{module_count} l{lambda} r{req1} r{req2}... n{source_node} f{failed1} f{failed2}...\n",
    "    # Note: 'order' needs to be added by the calling function\n",
    "    spec_line_parts = [\n",
    "        f\"m{module_count}\",\n",
    "        f\"l{lambda_val}\"\n",
    "    ]\n",
    "    spec_line_parts.extend(required_storage_list) # Use the renamed list\n",
    "    spec_line_parts.append(f\"n{source_node_id}\")\n",
    "    # Add failed nodes, ensuring they are prefixed with 'f' and sorted for consistency\n",
    "    spec_line_parts.extend(sorted([f\"f{int(node_id)}\" for node_id in all_failed_nodes_for_app]))\n",
    "    txt_line1_content = \" \".join(spec_line_parts)\n",
    "\n",
    "    # Line 2: n{placement1} n{placement2}...\n",
    "    txt_line2_content = \" \".join(placement_line_nodes)\n",
    "\n",
    "    # Return JSON allocation AND the content for the two TXT lines (without the order prefix)\n",
    "    return allocation, txt_line1_content, txt_line2_content\n",
    "\n",
    "# Modified create_placementFFNAPS2 to handle TXT file writing\n",
    "# Replaced 'RAM' with 'Storage'\n",
    "def create_placementFFNAPS2_Storage(data_pop, adjacency_dict, app_spec, node_spec, popul, app_id, cloud_node_id, save_folder):\n",
    "    \"\"\"\n",
    "    Main function to create placement using FFNAPS2 logic based on Storage.\n",
    "    Now saves both JSON and appends detailed TXT logs.\n",
    "    Renamed 'pop' to 'popul' and 'app' to 'app_id' for clarity.\n",
    "    Added 'save_folder' parameter. Uses 'Storage' instead of 'RAM'.\n",
    "    (Function name changed slightly to reflect Storage focus).\n",
    "    \"\"\"\n",
    "    JSONfile = {\"initialAllocation\": []}\n",
    "    all_txt_output_lines = [] # Collect all TXT lines here\n",
    "    order = 0  # Initialize order counter for TXT output\n",
    "\n",
    "    # Create save directory if it doesn't exist\n",
    "    try:\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory {save_folder}: {e}\")\n",
    "        return False # Cannot proceed without output directory\n",
    "\n",
    "    # print(\"Starting the placement process...\")\n",
    "\n",
    "    # Create a copy of node_spec to modify, preserving the original for potential reuse.\n",
    "    # This copy will track available 'Storage'.\n",
    "    local_node_spec = node_spec.copy()\n",
    "\n",
    "    for source in data_pop['sources']:\n",
    "        current_app_id = source['app']\n",
    "        # print(f\"\\nProcessing source for app: {current_app_id}, starting node: {source['id_resource']}\")\n",
    "        # Ensure app_spec has 'Storage' column\n",
    "        if 'Storage' not in app_spec.columns:\n",
    "             print(f\"Error: 'Storage' column missing in app_spec. Skipping app {current_app_id}\")\n",
    "             continue\n",
    "        app_specifications = app_spec[app_spec['app'] == current_app_id].drop_duplicates(subset=['Module', 'app'])\n",
    "\n",
    "        if app_specifications.empty:\n",
    "             # print(f\"Warning: No modules found for application {current_app_id} in app_spec. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        # Ensure local_node_spec has 'Storage' column\n",
    "        if 'Storage' not in local_node_spec.columns:\n",
    "            print(f\"Error: 'Storage' column missing in node_spec copy. Aborting.\")\n",
    "            return False\n",
    "\n",
    "        # Call the modified process_application (which now uses Storage)\n",
    "        allocation, txt_line1_content, txt_line2_content = process_application(\n",
    "            app_specifications, source, local_node_spec, adjacency_dict, cloud_node_id\n",
    "        )\n",
    "\n",
    "        if allocation is None: # Check if placement failed catastrophically (shouldn't with cloud fallback)\n",
    "             # print(f\"Placement failed critically for application {current_app_id}\")\n",
    "             continue\n",
    "\n",
    "        # Add results to overall JSON\n",
    "        JSONfile[\"initialAllocation\"].extend(allocation)\n",
    "\n",
    "        # Add formatted lines to TXT output list, prepending the order\n",
    "        all_txt_output_lines.append(f\"o{order} {txt_line1_content}\")\n",
    "        all_txt_output_lines.append(txt_line2_content)\n",
    "\n",
    "        order += 1 # Increment order for the next application\n",
    "\n",
    "    # --- Save JSON output ---\n",
    "    json_filename = f\"placement\\D100_P{popul}_A{app_id}_firstfit.json\" # Adjusted filename\n",
    "    json_filepath = os.path.join(save_folder, json_filename)\n",
    "    try:\n",
    "        with open(json_filepath, \"w\") as outfile:\n",
    "            json.dump(JSONfile, outfile, indent=2)\n",
    "        # print(f\"JSON placement saved to {json_filepath}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing JSON file {json_filepath}: {e}\")\n",
    "        # Decide if failure to write JSON should stop TXT writing or return False\n",
    "\n",
    "\n",
    "    # --- Save TXT output ---\n",
    "    txt_filename = f\"unselected_data\\D100_P{popul}_A{app_id}_firstfit.txt\" # Adjusted filename\n",
    "    txt_filepath = os.path.join(save_folder, txt_filename)\n",
    "    try:\n",
    "        # Check if file exists and is not empty to decide if newline needed before append\n",
    "        prepend_newline = os.path.exists(txt_filepath) and os.path.getsize(txt_filepath) > 0\n",
    "        with open(txt_filepath, \"a\") as outfile: # Use append mode 'a'\n",
    "            if prepend_newline:\n",
    "                outfile.write(\"\\n\")\n",
    "            outfile.write(\"\\n\".join(all_txt_output_lines))\n",
    "        # print(f\"TXT log appended to {txt_filepath}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error appending TXT file {txt_filepath}: {e}\")\n",
    "\n",
    "    # # Assuming checkthisout is a valid function using Storage\n",
    "    # print(checkthisout(json_filepath, app_spec, node_spec)) # Use json_filepath and ensure checkthisout expects 'Storage'\n",
    "\n",
    "    # print(\"\\nPlacement process finished.\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "execution_times = []\n",
    "methodname = \"firstfit_withfailure\"\n",
    "hopnumba = 2\n",
    "\n",
    "pop = 0\n",
    "app = 10\n",
    "\n",
    "f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\topology\\100m2.json')\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "# print(f)\n",
    "\n",
    "f = open(fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\population\\100m2_P{pop}_E{app}.json\")\n",
    "data_pop = json.load(f)\n",
    "# print(f)\n",
    "\n",
    "f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\application\\E{app}_{pop}.json')\n",
    "data_app = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "new_dict, source = create_dict(data)\n",
    "app_spec = app_specification(data_app)\n",
    "node_spec = node_specification(data)\n",
    "source = list(dict.fromkeys(source))\n",
    "pop_id = pop\n",
    "app_id = app\n",
    "\n",
    "cloud_node_id = 100\n",
    "\n",
    "save_folder = fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\testing\"\n",
    "# Time the execution of create_placementold\n",
    "start_time = time.time()\n",
    "create_placementFFNAPS2_Storage(data_pop, new_dict, app_spec, node_spec, pop_id, app_id, cloud_node_id, save_folder)\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "execution_times.append({'pop': pop, 'app': app, 'execution_time': execution_time, 'methodname': methodname})\n",
    "\n",
    "# Create DataFrame from execution times\n",
    "df_execution_times = pd.DataFrame(execution_times)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_execution_times.to_csv(f\"execution_times_{methodname}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## runing the loop firstfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "execution_times = []\n",
    "methodname = \"firstfit_withfailure\"\n",
    "hopnumba = 2\n",
    "\n",
    "for pop in range(10):\n",
    "    for app in range(10):\n",
    "\n",
    "\n",
    "        f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\topology\\100m2.json')\n",
    "        # a dictionary\n",
    "        data = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        f = open(fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\population\\100m2_P{pop}_D100.json\")\n",
    "        data_pop = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\application\\D100_{app}.json')\n",
    "        data_app = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        new_dict, source = create_dict(data)\n",
    "        app_spec = app_specification(data_app)\n",
    "        node_spec = node_specification(data)\n",
    "        source = list(dict.fromkeys(source))\n",
    "        pop_id = pop\n",
    "        app_id = app\n",
    "\n",
    "        cloud_node_id = 100\n",
    "\n",
    "        save_folder = fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\training_data\"\n",
    "        # Time the execution of create_placementold\n",
    "        start_time = time.time()\n",
    "        create_placementFFNAPS2_Storage(data_pop, new_dict, app_spec, node_spec, pop_id, app_id, cloud_node_id, save_folder)\n",
    "        end_time = time.time()\n",
    "\n",
    "        execution_time = end_time - start_time\n",
    "        execution_times.append({'pop': pop, 'app': app, 'execution_time': execution_time, 'methodname': methodname})\n",
    "\n",
    "# Create DataFrame from execution times\n",
    "df_execution_times = pd.DataFrame(execution_times)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_execution_times.to_csv(f\"execution_times_{methodname}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sort_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import os # Added for file path operations\n",
    "\n",
    "# Modified place_on_fastest_node for Storage and logging\n",
    "def place_module_fastest_first(row, node_spec, cloud_node_id, cloud_used):\n",
    "    \"\"\"\n",
    "    Places a module based on Storage availability on the fastest nodes first (sorted by Speed).\n",
    "    Handles cloud persistence and fallback. Tracks failed node attempts for logging.\n",
    "    Assumes node_spec index is the node 'id'.\n",
    "    \"\"\"\n",
    "    failed_nodes_for_this_call = [] # Track nodes failed in this call\n",
    "\n",
    "    # Check if cloud persistence forces placement here\n",
    "    if cloud_used:\n",
    "        # If any previous module of this app used cloud, force this one to cloud too\n",
    "        # print(f\"Placing Module {row['Module']} on cloud due to previous cloud usage.\")\n",
    "        try:\n",
    "            required_storage = int(row['Storage'])\n",
    "            # Directly modify the DataFrame passed in (which should be a copy)\n",
    "            storage_after_placement = node_spec.loc[cloud_node_id, 'Storage'] - required_storage\n",
    "            node_spec.loc[cloud_node_id, 'Storage'] = storage_after_placement\n",
    "            # No new nodes failed in this specific call if forced to cloud initially\n",
    "            return cloud_node_id, True, []\n",
    "        except (KeyError, ValueError, TypeError) as e:\n",
    "            print(f\"Error placing module {row['Module']} on forced cloud ({cloud_node_id}): {e}\")\n",
    "            # Decide on fallback? For now, assume cloud placement still happens conceptually\n",
    "            # Mark cloud as failed? Or return error? Let's return cloud ID but log the issue.\n",
    "            return cloud_node_id, True, [f\"f{cloud_node_id}_error\"] # Log error on cloud node\n",
    "\n",
    "    # If not forced to cloud, try fastest nodes first\n",
    "    try:\n",
    "        required_storage = int(row['Storage'])\n",
    "\n",
    "        # Sort available *non-cloud* nodes by 'Speed' (Instruction Per Tick?) descending\n",
    "        # Assumes 'Speed' column exists. Filter using index now.\n",
    "        non_cloud_nodes = node_spec[node_spec.index != cloud_node_id]\n",
    "        # Handle case where no non-cloud nodes exist\n",
    "        if non_cloud_nodes.empty:\n",
    "             # print(f\"No non-cloud nodes available. Placing Module {row['Module']} on cloud.\")\n",
    "             # Fall through to cloud placement logic below\n",
    "             pass # Allow logic to proceed to cloud fallback naturally\n",
    "        else:\n",
    "            sorted_node_ids = non_cloud_nodes.sort_values('Speed', ascending=False).index\n",
    "\n",
    "            for node_id in sorted_node_ids:\n",
    "                try:\n",
    "                    available_storage = node_spec.loc[node_id, 'Storage'] - required_storage\n",
    "                    if available_storage >= 0:\n",
    "                        # print(f\"Module {row['Module']} placed on fast node {node_id}.\")\n",
    "                        # Update the DataFrame copy\n",
    "                        node_spec.loc[node_id, 'Storage'] = available_storage\n",
    "                        # Return node, cloud_used=False, and nodes failed *before* this success\n",
    "                        return node_id, False, failed_nodes_for_this_call\n",
    "                    else:\n",
    "                        # Insufficient storage on this node, log it as failed for this attempt\n",
    "                        if node_id not in failed_nodes_for_this_call:\n",
    "                            failed_nodes_for_this_call.append(node_id)\n",
    "                except (KeyError, ValueError, TypeError) as e:\n",
    "                     # Error accessing this specific node, log it as failed\n",
    "                     print(f\"Error checking node {node_id} for module {row['Module']}: {e}\")\n",
    "                     if node_id not in failed_nodes_for_this_call:\n",
    "                            failed_nodes_for_this_call.append(f\"{node_id}_error\") # Append error marker\n",
    "\n",
    "        # If loop completes without returning, no suitable *non-cloud* node was found\n",
    "        # print(f\"No suitable fast node found for {row['Module']}. Placing on cloud.\")\n",
    "        # Update failed list: all checked non-cloud nodes failed\n",
    "        # Use the sorted list we tried, filter out any potential error markers already added\n",
    "        all_checked_ids = [nid for nid in sorted_node_ids if isinstance(nid, (int, float))] # Ensure numeric IDs\n",
    "        failed_nodes_for_this_call = all_checked_ids # All checked nodes failed\n",
    "\n",
    "    except (KeyError, ValueError, TypeError) as e:\n",
    "        print(f\"Error processing module {row['Module']} requirements or node sorting: {e}. Falling back to cloud.\")\n",
    "        # Treat as if all non-cloud nodes failed if error happened before/during sorting\n",
    "        try:\n",
    "             failed_nodes_for_this_call = list(node_spec[node_spec.index != cloud_node_id].index)\n",
    "        except: # Catch all if node_spec itself is problematic\n",
    "             failed_nodes_for_this_call = [\"f_error_processing_nodes\"]\n",
    "\n",
    "\n",
    "    # --- Cloud Placement Fallback ---\n",
    "    try:\n",
    "        required_storage = int(row['Storage']) # Recalculate in case of error above\n",
    "        storage_after_placement = node_spec.loc[cloud_node_id, 'Storage'] - required_storage\n",
    "        # Update the DataFrame copy\n",
    "        node_spec.loc[cloud_node_id, 'Storage'] = storage_after_placement\n",
    "        return cloud_node_id, True, failed_nodes_for_this_call # Return cloud, set flag, return failed nodes\n",
    "    except (KeyError, ValueError, TypeError) as e:\n",
    "        print(f\"CRITICAL Error placing module {row['Module']} on fallback cloud ({cloud_node_id}): {e}\")\n",
    "        # If cloud itself fails, something is very wrong. Return cloud ID but flag error.\n",
    "        return cloud_node_id, True, failed_nodes_for_this_call + [f\"f{cloud_node_id}_critical_error\"]\n",
    "\n",
    "\n",
    "# Modified process_application_fastest_nodes for Storage and logging\n",
    "def process_application_fastest_nodes(app_specifications, source, node_spec, cloud_node_id):\n",
    "    \"\"\"\n",
    "    Processes modules of an application using the 'fastest node first' strategy based on Storage.\n",
    "    Collects data for TXT logging and returns formatted TXT lines.\n",
    "    Modifies the passed node_spec (which should be a copy).\n",
    "    \"\"\"\n",
    "    allocation = [] # For JSON output\n",
    "    placement_line_nodes = [] # For TXT: stores n{id} placements\n",
    "    required_storage_list = [] # For TXT: stores r{storage} requirements\n",
    "    all_failed_nodes_for_app = set() # For TXT: stores unique f{id} failed nodes for this app\n",
    "\n",
    "    # Get required info from source for TXT header\n",
    "    app_name = source['app']\n",
    "    source_node_id = source['id_resource'] # Keep track of original source for logging\n",
    "    lambda_val = source.get('lambda', 'N/A') # Safely get lambda\n",
    "    module_count = len(app_specifications)\n",
    "\n",
    "    cloud_used = False  # Flag to track if *any* module of *this app* is placed on cloud\n",
    "\n",
    "    # print(f\"Processing application: {app_name} from source {source_node_id} (Fastest Node Strategy)\")\n",
    "\n",
    "    for _, row in app_specifications.iterrows():\n",
    "        module_name = row['Module']\n",
    "        try:\n",
    "            required_storage = int(row['Storage']) # Assume Storage is integer\n",
    "            required_storage_list.append(f\"r{required_storage}\") # Store for TXT spec line\n",
    "        except (KeyError, ValueError, TypeError):\n",
    "             print(f\"Warning: Invalid Storage for module {module_name} (App {app_name}). Skipping module.\")\n",
    "             required_storage_list.append(\"r?_invalid\")\n",
    "             placement_line_nodes.append(\"n?_skipped\")\n",
    "             # Should we mark cloud used? Let's assume not for a skipped module.\n",
    "             continue # Skip to next module\n",
    "\n",
    "        # Call modified placement function\n",
    "        resource_id, cloud_used_flag_updated, failed_nodes_for_this_call = place_module_fastest_first(\n",
    "            row, node_spec, cloud_node_id, cloud_used\n",
    "        )\n",
    "\n",
    "        # Update the overall cloud_used flag for the application\n",
    "        cloud_used = cloud_used_flag_updated\n",
    "\n",
    "        # Update the set of all failed nodes encountered for this application\n",
    "        # Ensure we handle potential string markers like 'id_error'\n",
    "        for node in failed_nodes_for_this_call:\n",
    "             all_failed_nodes_for_app.add(node) # Add directly, set handles uniqueness\n",
    "\n",
    "        # Record placement for JSON\n",
    "        placement = {\"module_name\": module_name, \"app\": app_name, \"id_resource\": resource_id}\n",
    "        allocation.append(placement)\n",
    "\n",
    "        # Record placement for TXT\n",
    "        placement_line_nodes.append(f\"n{resource_id}\")\n",
    "        # print(f\"Module {module_name} placed on node {resource_id}. Cloud used so far for app: {cloud_used}\")\n",
    "\n",
    "\n",
    "    # --- Format TXT output lines for this application ---\n",
    "    # Line 1: o{order} m{module_count} l{lambda} r{req1} r{req2}... n{source_node} f{failed1} f{failed2}...\n",
    "    spec_line_parts = [\n",
    "        f\"m{module_count}\",\n",
    "        f\"l{lambda_val}\"\n",
    "    ]\n",
    "    spec_line_parts.extend(required_storage_list)\n",
    "    spec_line_parts.append(f\"n{source_node_id}\") # Use original source node ID\n",
    "    # Add failed nodes, prefixing numeric IDs with 'f', keeping error strings as is, and sorting\n",
    "    formatted_failed_nodes = []\n",
    "    for node in all_failed_nodes_for_app:\n",
    "        try:\n",
    "            # Try converting to int to check if it's a pure node ID\n",
    "            int_node = int(node)\n",
    "            formatted_failed_nodes.append(f\"f{int_node}\")\n",
    "        except (ValueError, TypeError):\n",
    "            # If conversion fails, it's likely an error string like 'id_error' or 'fX_error'\n",
    "            formatted_failed_nodes.append(str(node)) # Keep the error string\n",
    "    spec_line_parts.extend(sorted(formatted_failed_nodes))\n",
    "    txt_line1_content = \" \".join(spec_line_parts)\n",
    "\n",
    "    # Line 2: n{placement1} n{placement2}...\n",
    "    txt_line2_content = \" \".join(placement_line_nodes)\n",
    "\n",
    "    # Return JSON allocation AND the content for the two TXT lines (without the order prefix)\n",
    "    return allocation, txt_line1_content, txt_line2_content\n",
    "\n",
    "\n",
    "# Modified create_placementTanejaModified for Storage and logging\n",
    "def create_placementTanejaModified(data_pop, app_spec, node_spec, popul, app_id, cloud_node_id, save_folder):\n",
    "    \"\"\"\n",
    "    Main function to create placement using Taneja Modified (Fastest Node First) logic based on Storage.\n",
    "    Saves both JSON and appends detailed TXT logs.\n",
    "    Uses 'Storage' instead of 'RAM'. Assumes 'Speed' column exists in node_spec.\n",
    "    Removed unused 'new_dict' parameter.\n",
    "    \"\"\"\n",
    "    JSONfile = {\"initialAllocation\": []}\n",
    "    all_txt_output_lines = [] # Collect all TXT lines here\n",
    "    order = 0  # Initialize order counter for TXT output\n",
    "\n",
    "    # Create save directory if it doesn't exist\n",
    "    try:\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory {save_folder}: {e}\")\n",
    "        return False # Cannot proceed without output directory\n",
    "\n",
    "    # print(\"Starting the Taneja Modified (fastest first) placement process...\")\n",
    "\n",
    "    # --- Create a local copy of node_spec to modify ---\n",
    "    # Ensure the index is 'id' for consistent access in helper functions\n",
    "    local_node_spec = node_spec.copy()\n",
    "    if 'id' not in local_node_spec.columns and not pd.api.types.is_integer_dtype(local_node_spec.index):\n",
    "         print(\"Error: node_spec must have an 'id' column or an integer index.\")\n",
    "         return False\n",
    "    if 'id' in local_node_spec.columns and not pd.api.types.is_integer_dtype(local_node_spec.index):\n",
    "        local_node_spec = local_node_spec.set_index('id', drop=False) # Keep id column if needed elsewhere too\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if 'Storage' not in local_node_spec.columns:\n",
    "        print(\"Error: 'Storage' column missing in node_spec.\")\n",
    "        return False\n",
    "    if 'Speed' not in local_node_spec.columns:\n",
    "        print(\"Error: 'Speed' column missing in node_spec (required for sorting).\")\n",
    "        return False\n",
    "    if 'Storage' not in app_spec.columns:\n",
    "        print(\"Error: 'Storage' column missing in app_spec.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "    for source in data_pop['sources']:\n",
    "        current_app_id = source['app']\n",
    "        # print(f\"\\nProcessing source for app: {current_app_id}, initial source node: {source['id_resource']}\")\n",
    "        app_specifications = app_spec[app_spec['app'] == current_app_id].drop_duplicates(subset=['Module', 'app'])\n",
    "\n",
    "        if app_specifications.empty:\n",
    "             # print(f\"Warning: No modules found for application {current_app_id} in app_spec. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        # Call the modified process_application function, passing the local copy\n",
    "        allocation, txt_line1_content, txt_line2_content = process_application_fastest_nodes(\n",
    "            app_specifications, source, local_node_spec, cloud_node_id\n",
    "        )\n",
    "\n",
    "        if allocation is None: # Should not happen given the logic, but check anyway\n",
    "             print(f\"Placement failed critically for application {current_app_id}\")\n",
    "             continue\n",
    "\n",
    "        # Add results to overall JSON\n",
    "        JSONfile[\"initialAllocation\"].extend(allocation)\n",
    "\n",
    "        # Add formatted lines to TXT output list, prepending the order\n",
    "        all_txt_output_lines.append(f\"o{order} {txt_line1_content}\")\n",
    "        all_txt_output_lines.append(txt_line2_content)\n",
    "\n",
    "        order += 1 # Increment order for the next application\n",
    "\n",
    "    # --- Save JSON output ---\n",
    "    json_filename = f\"placement\\D100_P{popul}_A{app_id}_sortmatch.json\" # Adjusted filename\n",
    "    json_filepath = os.path.join(save_folder, json_filename)\n",
    "    try:\n",
    "        with open(json_filepath, \"w\") as outfile:\n",
    "            json.dump(JSONfile, outfile, indent=2)\n",
    "        # print(f\"JSON placement saved to {json_filepath}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing JSON file {json_filepath}: {e}\")\n",
    "\n",
    "    # --- Save TXT output ---\n",
    "    txt_filename = f\"D100_P{popul}_A{app_id}_sortmatch.txt\" # Adjusted filename\n",
    "    txt_filepath = os.path.join(save_folder, txt_filename)\n",
    "    try:\n",
    "        # Check if file exists and is not empty to decide if newline needed before append\n",
    "        prepend_newline = os.path.exists(txt_filepath) and os.path.getsize(txt_filepath) > 0\n",
    "        with open(txt_filepath, \"a\") as outfile: # Use append mode 'a'\n",
    "            if prepend_newline:\n",
    "                outfile.write(\"\\n\")\n",
    "            outfile.write(\"\\n\".join(all_txt_output_lines))\n",
    "        # print(f\"TXT log appended to {txt_filepath}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error appending TXT file {txt_filepath}: {e}\")\n",
    "\n",
    "    # print(\"\\nPlacement process finished.\")\n",
    "    return True\n",
    "\n",
    "# --- Example Usage Setup (Updated for Storage and Speed) ---\n",
    "# Example DataFrames and Dictionaries (replace with your actual data loading)\n",
    "# node_spec = pd.DataFrame({\n",
    "#     'id': [1, 2, 3, 100],\n",
    "#     'Storage': [1000, 800, 1200, 10000], # Using Storage\n",
    "#     'Speed': [500, 1500, 1000, 100] # Added Speed (higher is faster?)\n",
    "# }).set_index('id') # Set 'id' as index\n",
    "# app_spec = pd.DataFrame({\n",
    "#     'app': ['App1', 'App1', 'App2'],\n",
    "#     'Module': ['M1', 'M2', 'M3'],\n",
    "#     'Storage': [400, 500, 700] # Using Storage\n",
    "# })\n",
    "# data_pop = {\n",
    "#     'sources': [\n",
    "#         {'app': 'App1', 'id_resource': 1, 'lambda': 0.5}, # Source node isn't directly used by placement logic here\n",
    "#         {'app': 'App2', 'id_resource': 2, 'lambda': 0.8}\n",
    "#     ]\n",
    "# }\n",
    "# # adjacency_dict (new_dict) is not used by this logic, so removed from params\n",
    "# cloud_node_id = 100\n",
    "# popul = 1\n",
    "# app_id = \"SetB\"\n",
    "# tipe = \"FastestTestRun\"\n",
    "# save_folder = \"fastest_storage_output\" # Adjusted output folder name\n",
    "\n",
    "# --- Call the main function ---\n",
    "# Ensure your node_spec and app_spec DataFrames have 'Storage' columns and node_spec has 'Speed'\n",
    "# success = create_placementTanejaModified_Storage(data_pop, app_spec, node_spec, popul, app_id, tipe, cloud_node_id, save_folder)\n",
    "# print(f\"Placement successful: {success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "execution_times = []\n",
    "methodname = \"sortmatch\"\n",
    "hopnumba = 2\n",
    "\n",
    "pop = 0\n",
    "app = 10\n",
    "\n",
    "f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\topology\\100m2.json')\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "# print(f)\n",
    "\n",
    "f = open(fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\population\\100m2_P{pop}_E{app}.json\")\n",
    "data_pop = json.load(f)\n",
    "# print(f)\n",
    "\n",
    "f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\application\\E{app}_{pop}.json')\n",
    "data_app = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "new_dict, source = create_dict(data)\n",
    "app_spec = app_specification(data_app)\n",
    "node_spec = node_specification(data)\n",
    "source = list(dict.fromkeys(source))\n",
    "pop_id = pop\n",
    "app_id = app\n",
    "\n",
    "cloud_node_id = 100\n",
    "\n",
    "save_folder = fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\testing\"\n",
    "# Time the execution of create_placementold\n",
    "start_time = time.time()\n",
    "create_placementTanejaModified(data_pop, app_spec, node_spec, pop_id, app_id, cloud_node_id, save_folder)\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "execution_times.append({'pop': pop, 'app': app, 'execution_time': execution_time, 'methodname': methodname})\n",
    "\n",
    "# Create DataFrame from execution times\n",
    "df_execution_times = pd.DataFrame(execution_times)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_execution_times.to_csv(f\"execution_times_{methodname}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running the loop sortmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "execution_times = []\n",
    "methodname = \"sortmatch_withfailure\"\n",
    "hopnumba = 2\n",
    "\n",
    "for pop in range(10):\n",
    "    for app in range(10):\n",
    "\n",
    "\n",
    "        f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\topology\\100m2.json')\n",
    "        # a dictionary\n",
    "        data = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        f = open(fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\population\\100m2_P{pop}_D100.json\")\n",
    "        data_pop = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\application\\D100_{app}.json')\n",
    "        data_app = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        new_dict, source = create_dict(data)\n",
    "        app_spec = app_specification(data_app)\n",
    "        node_spec = node_specification(data)\n",
    "        source = list(dict.fromkeys(source))\n",
    "        pop_id = pop\n",
    "        app_id = app\n",
    "\n",
    "        cloud_node_id = 100\n",
    "\n",
    "        save_folder = fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\training_data\"\n",
    "        # Time the execution of create_placementold\n",
    "        start_time = time.time()\n",
    "        create_placementTanejaModified(data_pop, app_spec, node_spec, pop_id, app_id, cloud_node_id, save_folder)\n",
    "        end_time = time.time()\n",
    "\n",
    "        execution_time = end_time - start_time\n",
    "        execution_times.append({'pop': pop, 'app': app, 'execution_time': execution_time, 'methodname': methodname})\n",
    "\n",
    "# Create DataFrame from execution times\n",
    "df_execution_times = pd.DataFrame(execution_times)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_execution_times.to_csv(f\"execution_times_{methodname}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skarlt_firstfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Modified find_suitable_node to return failed attempts for logging\n",
    "# Replaced 'RAM' with 'Storage'\n",
    "def find_suitable_node(row, node_spec, current_resource_id, adjacency_dict, cloud_node_id):\n",
    "    \"\"\"\n",
    "    Find a suitable node for module placement based on Storage with a fallback to cloud node.\n",
    "    Now also returns a list of nodes that failed due to insufficient Storage during this search.\n",
    "    \"\"\"\n",
    "    failed_nodes_for_this_call = [] # Track nodes that failed in this specific search\n",
    "    original_start_node = current_resource_id # Keep track of where this search started\n",
    "\n",
    "    for retry in range(5):  # retry up to 5 times\n",
    "        search_path_this_retry = [] # Track path within a retry to avoid adding duplicates quickly\n",
    "        temp_resource_id = original_start_node if retry == 0 else current_resource_id # Start fresh or continue from last random hop\n",
    "\n",
    "        for attempt in range(5):  # try up to 5 times per retry\n",
    "            available_storage = -1 # Default to -1 if node ID is invalid or not found\n",
    "            node_to_check = int(temp_resource_id) # Ensure integer ID\n",
    "\n",
    "            if node_to_check in node_spec.index:\n",
    "                 # Use .loc for safe access using 'Storage' column\n",
    "                available_storage = node_spec.loc[node_to_check, 'Storage'] - row['Storage']\n",
    "                # print(f\"Retry {retry + 1}, attempt {attempt + 1} for module {row['Module']} on node {node_to_check}.\")\n",
    "                # print(f\"Node {node_to_check} has {node_spec.loc[node_to_check, 'Storage']} Storage, app needed {row['Storage']}, available Storage after placement {available_storage}\")\n",
    "            else:\n",
    "                # print(f\"Warning: Node {node_to_check} not found in node_spec index during search.\")\n",
    "                # Treat as failed node? Or rely on random choice moving away?\n",
    "                # Let's log it as failed and try moving away.\n",
    "                 if node_to_check not in failed_nodes_for_this_call:\n",
    "                      failed_nodes_for_this_call.append(node_to_check)\n",
    "                 # Try to pick a neighbor of the *previous valid node* if possible, or fall back\n",
    "                 # This part of the original logic might need refinement if node IDs can be invalid often\n",
    "                 # For now, let's try to move from the last known *good* or attempted node in the path if possible\n",
    "                 last_known_node = search_path_this_retry[-1] if search_path_this_retry else original_start_node\n",
    "                 try:\n",
    "                     temp_resource_id = random.choice(adjacency_dict.get(int(last_known_node), [cloud_node_id])) # Default to cloud if no neighbors\n",
    "                 except (ValueError, TypeError):\n",
    "                     temp_resource_id = cloud_node_id # Fallback if ID conversion fails\n",
    "                 continue # Skip to next attempt\n",
    "\n",
    "            if available_storage >= 0:\n",
    "                # print(\"Suitable placement found.\")\n",
    "                # Return chosen node, remaining Storage, and the list of nodes failed *during this search*\n",
    "                return node_to_check, available_storage, failed_nodes_for_this_call\n",
    "\n",
    "            # If here, placement failed on node_to_check due to insufficient Storage\n",
    "            if node_to_check not in failed_nodes_for_this_call:\n",
    "                 failed_nodes_for_this_call.append(node_to_check)\n",
    "            search_path_this_retry.append(node_to_check)\n",
    "\n",
    "            # Move to a random neighbor for the next attempt (original logic)\n",
    "            try:\n",
    "                # Ensure neighbors exist and handle potential errors\n",
    "                neighbors = adjacency_dict.get(node_to_check, [])\n",
    "                if not neighbors:\n",
    "                    # print(f\"Node {node_to_check} has no neighbors. Jumping to cloud for next attempt.\")\n",
    "                    temp_resource_id = cloud_node_id # Jump to cloud if no neighbors\n",
    "                else:\n",
    "                    temp_resource_id = random.choice(neighbors)\n",
    "                # print(f\"Not enough Storage on {node_to_check}. Trying a new node: {temp_resource_id}\")\n",
    "            except (ValueError, TypeError, KeyError) as e:\n",
    "                 # print(f\"Error finding neighbors for {node_to_check}: {e}. Jumping to cloud.\")\n",
    "                 temp_resource_id = cloud_node_id # Fallback to cloud\n",
    "\n",
    "        # After 5 attempts, select a new resource_id randomly for the next retry (original logic)\n",
    "        # Use the last attempted node (temp_resource_id) as the base for the next random hop\n",
    "        try:\n",
    "            neighbors = adjacency_dict.get(int(temp_resource_id), [])\n",
    "            if not neighbors:\n",
    "                # print(f\"Node {temp_resource_id} (end of retry {retry+1}) has no neighbors. Retrying from cloud.\")\n",
    "                current_resource_id = cloud_node_id\n",
    "            else:\n",
    "                current_resource_id = random.choice(neighbors)\n",
    "            # print(f\"Retrying with a new node: {current_resource_id} after 5 failed attempts in retry {retry + 1}.\")\n",
    "        except (ValueError, TypeError, KeyError) as e:\n",
    "            # print(f\"Error finding neighbors for {temp_resource_id} before retry: {e}. Retrying from cloud.\")\n",
    "            current_resource_id = cloud_node_id # Fallback to cloud\n",
    "\n",
    "    # If suitable placement is not found after 5 retries, place on cloud node (original logic)\n",
    "    # print(\"Failed to find a suitable edge node after 5 retries. Placing on cloud node.\")\n",
    "    # Return cloud node, its *original* Storage (calling function handles subtraction), and collected failed nodes\n",
    "    return cloud_node_id, node_spec.loc[cloud_node_id, 'Storage'], failed_nodes_for_this_call\n",
    "\n",
    "# Modified process_application to handle logging data collection and return TXT lines\n",
    "# Replaced 'RAM' with 'Storage'\n",
    "def process_application(app_specifications, source, node_spec, adjacency_dict, cloud_node_id):\n",
    "    \"\"\"\n",
    "    Process each application for placement based on Storage with cloud node fallback.\n",
    "    Now collects data for TXT logging and returns formatted TXT lines.\n",
    "    \"\"\"\n",
    "    allocation = [] # For JSON output\n",
    "    placement_line_nodes = [] # For TXT: stores n{id} placements\n",
    "    required_storage_list = [] # For TXT: stores r{storage} requirements\n",
    "    all_failed_nodes_for_app = set() # For TXT: stores unique f{id} failed nodes for this app\n",
    "\n",
    "    # Get required info from source for TXT header\n",
    "    app_name = source['app']\n",
    "    source_node_id = source['id_resource']\n",
    "    lambda_val = source.get('lambda', 'N/A') # Safely get lambda\n",
    "    module_count = len(app_specifications)\n",
    "\n",
    "    # print(f\"Processing application: {app_name} from source {source_node_id}\")\n",
    "    # Initial resource_id for the *first* module search comes from the source\n",
    "    current_search_start_node = source_node_id\n",
    "\n",
    "    for _, row in app_specifications.iterrows():\n",
    "        module_name = row['Module']\n",
    "        required_storage = int(row['Storage']) # Assume Storage is integer\n",
    "        required_storage_list.append(f\"r{required_storage}\") # Store for TXT spec line\n",
    "\n",
    "        # Call modified find_suitable_node\n",
    "        # Pass the node where the *previous* module was placed (or source node initially)\n",
    "        # as the starting point for the search\n",
    "        resource_id, available_storage_after_placement, failed_nodes_for_this_call = find_suitable_node(\n",
    "            row, node_spec, current_search_start_node, adjacency_dict, cloud_node_id\n",
    "        )\n",
    "\n",
    "        # Update the set of all failed nodes encountered for this application\n",
    "        all_failed_nodes_for_app.update(failed_nodes_for_this_call)\n",
    "\n",
    "        # Update node_spec Storage (original logic adapted)\n",
    "        # If placed on cloud, available_storage_after_placement is total cloud Storage, so subtract now\n",
    "        if resource_id == cloud_node_id:\n",
    "            # print(f\"Module {module_name} placed on Cloud ({cloud_node_id}). Updating cloud Storage.\")\n",
    "            node_spec.loc[resource_id, 'Storage'] -= required_storage\n",
    "        else:\n",
    "            # print(f\"Module {module_name} placed on Node {resource_id}. Updating node Storage.\")\n",
    "            node_spec.loc[resource_id, 'Storage'] = available_storage_after_placement # This was already calculated correctly\n",
    "\n",
    "        # Record placement for JSON\n",
    "        placement = {\"module_name\": module_name, \"app\": app_name, \"id_resource\": resource_id}\n",
    "        allocation.append(placement)\n",
    "\n",
    "        # Record placement for TXT\n",
    "        placement_line_nodes.append(f\"n{resource_id}\")\n",
    "        # print(f\"Module {module_name} placed on node {resource_id}. Available Storage left: {node_spec.loc[resource_id, 'Storage']}\")\n",
    "\n",
    "        # The node where this module was placed becomes the starting point for the next module's search\n",
    "        current_search_start_node = resource_id\n",
    "\n",
    "    # --- Format TXT output lines for this application ---\n",
    "    # Line 1: o{order} m{module_count} l{lambda} r{req1} r{req2}... n{source_node} f{failed1} f{failed2}...\n",
    "    # Note: 'order' needs to be added by the calling function\n",
    "    spec_line_parts = [\n",
    "        f\"m{module_count}\",\n",
    "        f\"l{lambda_val}\"\n",
    "    ]\n",
    "    spec_line_parts.extend(required_storage_list) # Use the renamed list\n",
    "    spec_line_parts.append(f\"n{source_node_id}\")\n",
    "    # Add failed nodes, ensuring they are prefixed with 'f' and sorted for consistency\n",
    "    spec_line_parts.extend(sorted([f\"f{int(node_id)}\" for node_id in all_failed_nodes_for_app]))\n",
    "    txt_line1_content = \" \".join(spec_line_parts)\n",
    "\n",
    "    # Line 2: n{placement1} n{placement2}...\n",
    "    txt_line2_content = \" \".join(placement_line_nodes)\n",
    "\n",
    "    # Return JSON allocation AND the content for the two TXT lines (without the order prefix)\n",
    "    return allocation, txt_line1_content, txt_line2_content\n",
    "\n",
    "# Modified create_placementFFNAPS2 to handle TXT file writing\n",
    "# Replaced 'RAM' with 'Storage'\n",
    "def create_placementskarlatFF_Storage(data_pop, adjacency_dict, app_spec, node_spec, popul, app_id, cloud_node_id, save_folder):\n",
    "    \"\"\"\n",
    "    Main function to create placement using FFNAPS2 logic based on Storage.\n",
    "    Now saves both JSON and appends detailed TXT logs.\n",
    "    Renamed 'pop' to 'popul' and 'app' to 'app_id' for clarity.\n",
    "    Added 'save_folder' parameter. Uses 'Storage' instead of 'RAM'.\n",
    "    (Function name changed slightly to reflect Storage focus).\n",
    "    \"\"\"\n",
    "    JSONfile = {\"initialAllocation\": []}\n",
    "    all_txt_output_lines = [] # Collect all TXT lines here\n",
    "    order = 0  # Initialize order counter for TXT output\n",
    "\n",
    "    # Create save directory if it doesn't exist\n",
    "    try:\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory {save_folder}: {e}\")\n",
    "        return False # Cannot proceed without output directory\n",
    "\n",
    "    # print(\"Starting the placement process...\")\n",
    "\n",
    "    # Create a copy of node_spec to modify, preserving the original for potential reuse.\n",
    "    # This copy will track available 'Storage'.\n",
    "    local_node_spec = node_spec.copy()\n",
    "\n",
    "    for source in data_pop['sources']:\n",
    "        current_app_id = source['app']\n",
    "        # print(f\"\\nProcessing source for app: {current_app_id}, starting node: {source['id_resource']}\")\n",
    "        # Ensure app_spec has 'Storage' column\n",
    "        if 'Storage' not in app_spec.columns:\n",
    "             print(f\"Error: 'Storage' column missing in app_spec. Skipping app {current_app_id}\")\n",
    "             continue\n",
    "        app_specifications = app_spec[app_spec['app'] == current_app_id].drop_duplicates(subset=['Module', 'app'])\n",
    "\n",
    "        if app_specifications.empty:\n",
    "             # print(f\"Warning: No modules found for application {current_app_id} in app_spec. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        # Ensure local_node_spec has 'Storage' column\n",
    "        if 'Storage' not in local_node_spec.columns:\n",
    "            print(f\"Error: 'Storage' column missing in node_spec copy. Aborting.\")\n",
    "            return False\n",
    "\n",
    "        # Call the modified process_application (which now uses Storage)\n",
    "        allocation, txt_line1_content, txt_line2_content = process_application(\n",
    "            app_specifications, source, local_node_spec, adjacency_dict, cloud_node_id\n",
    "        )\n",
    "\n",
    "        if allocation is None: # Check if placement failed catastrophically (shouldn't with cloud fallback)\n",
    "             # print(f\"Placement failed critically for application {current_app_id}\")\n",
    "             continue\n",
    "\n",
    "        # Add results to overall JSON\n",
    "        JSONfile[\"initialAllocation\"].extend(allocation)\n",
    "\n",
    "        # Add formatted lines to TXT output list, prepending the order\n",
    "        all_txt_output_lines.append(f\"o{order} {txt_line1_content}\")\n",
    "        all_txt_output_lines.append(txt_line2_content)\n",
    "\n",
    "        order += 1 # Increment order for the next application\n",
    "\n",
    "    # --- Save JSON output ---\n",
    "    json_filename = f\"placement\\D100_P{popul}_A{app_id}_clusterfirstfit.json\" # Adjusted filename\n",
    "    json_filepath = os.path.join(save_folder, json_filename)\n",
    "    try:\n",
    "        with open(json_filepath, \"w\") as outfile:\n",
    "            json.dump(JSONfile, outfile, indent=2)\n",
    "        # print(f\"JSON placement saved to {json_filepath}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing JSON file {json_filepath}: {e}\")\n",
    "        # Decide if failure to write JSON should stop TXT writing or return False\n",
    "\n",
    "\n",
    "    # --- Save TXT output ---\n",
    "    txt_filename = f\"D100_P{popul}_A{app_id}_clusterfirstfit.txt\" # Adjusted filename\n",
    "    txt_filepath = os.path.join(save_folder, txt_filename)\n",
    "    try:\n",
    "        # Check if file exists and is not empty to decide if newline needed before append\n",
    "        prepend_newline = os.path.exists(txt_filepath) and os.path.getsize(txt_filepath) > 0\n",
    "        with open(txt_filepath, \"a\") as outfile: # Use append mode 'a'\n",
    "            if prepend_newline:\n",
    "                outfile.write(\"\\n\")\n",
    "            outfile.write(\"\\n\".join(all_txt_output_lines))\n",
    "        # print(f\"TXT log appended to {txt_filepath}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error appending TXT file {txt_filepath}: {e}\")\n",
    "\n",
    "    # # Assuming checkthisout is a valid function using Storage\n",
    "    # print(checkthisout(json_filepath, app_spec, node_spec)) # Use json_filepath and ensure checkthisout expects 'Storage'\n",
    "\n",
    "    # print(\"\\nPlacement process finished.\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map_skarlat(topo):\n",
    "    new_dict = dict()\n",
    "    source = []\n",
    "\n",
    "    for i in topo['links']:\n",
    "        source_node = i['source']\n",
    "        target_node = i['target']\n",
    "        source_type = topo['nodes'][source_node]['label']\n",
    "        target_type = topo['nodes'][target_node]['label']\n",
    "\n",
    "        # Rule for gateway: connect only to nodes (not node heads)\n",
    "        if source_type == 'gateway':\n",
    "            new_dict = add_connection(new_dict, source_node, target_node)\n",
    "            source.append(source_node)\n",
    "\n",
    "        # Rule for nodes: connect to other nodes and node heads\n",
    "        elif source_type != 'gateway' and source_type != 'cloud':\n",
    "            new_dict = add_connection(new_dict, source_node, target_node)\n",
    "            if target_type != 'gateway' and target_type != 'cloud':\n",
    "                new_dict = add_connection(new_dict, target_node, source_node)\n",
    "            source.append(source_node)\n",
    "\n",
    "        # Rule for node heads: connect only to other node heads\n",
    "        elif source_type == 'head_node' and target_type == 'head_node':\n",
    "            new_dict = add_connection(new_dict, source_node, target_node)\n",
    "            new_dict = add_connection(new_dict, target_node, source_node)\n",
    "            source.append(source_node)\n",
    "\n",
    "    for i in topo['links']:\n",
    "        source_node = i['target']\n",
    "        target_node = i['source']\n",
    "        source_type = topo['nodes'][source_node]['label']\n",
    "        target_type = topo['nodes'][target_node]['label']\n",
    "\n",
    "        # Rule for gateway: connect only to nodes (not node heads)\n",
    "        if source_type == 'gateway':\n",
    "            new_dict = add_connection(new_dict, source_node, target_node)\n",
    "            source.append(source_node)\n",
    "\n",
    "        # Rule for nodes: connect to other nodes and node heads\n",
    "        elif source_type != 'gateway' and source_type != 'cloud':\n",
    "            new_dict = add_connection(new_dict, source_node, target_node)\n",
    "            if target_type != 'gateway' and target_type != 'cloud':\n",
    "                new_dict = add_connection(new_dict, target_node, source_node)\n",
    "            source.append(source_node)\n",
    "\n",
    "        # Rule for node heads: connect only to other node heads\n",
    "        elif source_type == 'head_node' and target_type == 'head_node':\n",
    "            new_dict = add_connection(new_dict, source_node, target_node)\n",
    "            new_dict = add_connection(new_dict, target_node, source_node)\n",
    "            source.append(source_node)\n",
    "\n",
    "\n",
    "\n",
    "    return new_dict, source\n",
    "\n",
    "def add_connection(graph_dict, src, tgt):\n",
    "    \"\"\" Helper function to add a connection to the graph dictionary \"\"\"\n",
    "    if src in graph_dict:\n",
    "        graph_dict[src].append(tgt)\n",
    "    else:\n",
    "        graph_dict[src] = [tgt]\n",
    "    return graph_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing clusterfirstfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "execution_times = []\n",
    "methodname = \"clusterfirstfit\"\n",
    "hopnumba = 2\n",
    "\n",
    "pop = 0\n",
    "app = 10\n",
    "\n",
    "f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\topology\\100m2.json')\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "# print(f)\n",
    "\n",
    "f = open(fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\population\\100m2_P{pop}_E{app}.json\")\n",
    "data_pop = json.load(f)\n",
    "# print(f)\n",
    "\n",
    "f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\application\\E{app}_{pop}.json')\n",
    "data_app = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "new_dict, source = create_map_skarlat(data)\n",
    "app_spec = app_specification(data_app)\n",
    "node_spec = node_specification(data)\n",
    "source = list(dict.fromkeys(source))\n",
    "pop_id = pop\n",
    "app_id = app\n",
    "\n",
    "cloud_node_id = 100\n",
    "\n",
    "save_folder = fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\testing\"\n",
    "# Time the execution of create_placementold\n",
    "start_time = time.time()\n",
    "create_placementskarlatFF_Storage(data_pop, new_dict, app_spec, node_spec, pop_id, app_id, cloud_node_id, save_folder)\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "execution_times.append({'pop': pop, 'app': app, 'execution_time': execution_time, 'methodname': methodname})\n",
    "\n",
    "# Create DataFrame from execution times\n",
    "df_execution_times = pd.DataFrame(execution_times)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_execution_times.to_csv(f\"execution_times_{methodname}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running the loop clusterfirstfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "execution_times = []\n",
    "methodname = \"clusterfirstfit_withfailure\"\n",
    "hopnumba = 2\n",
    "\n",
    "for pop in range(10):\n",
    "    for app in range(10):\n",
    "\n",
    "\n",
    "        f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\topology\\100m2.json')\n",
    "        # a dictionary\n",
    "        data = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        f = open(fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\population\\100m2_P{pop}_D100.json\")\n",
    "        data_pop = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        f = open(fr'C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\application\\D100_{app}.json')\n",
    "        data_app = json.load(f)\n",
    "        # print(f)\n",
    "\n",
    "        new_dict, source = create_map_skarlat(data)\n",
    "        app_spec = app_specification(data_app)\n",
    "        node_spec = node_specification(data)\n",
    "        source = list(dict.fromkeys(source))\n",
    "        pop_id = pop\n",
    "        app_id = app\n",
    "\n",
    "        cloud_node_id = 100\n",
    "\n",
    "        save_folder = fr\"C:\\Users\\komputer\\OneDrive\\Documents\\GitHub\\MOFPSS-Seq2seq\\evaluation_files\\training_data\"\n",
    "        # Time the execution of create_placementold\n",
    "        start_time = time.time()\n",
    "        create_placementskarlatFF_Storage(data_pop, new_dict, app_spec, node_spec, pop_id, app_id, cloud_node_id, save_folder)\n",
    "        end_time = time.time()\n",
    "\n",
    "        execution_time = end_time - start_time\n",
    "        execution_times.append({'pop': pop, 'app': app, 'execution_time': execution_time, 'methodname': methodname})\n",
    "\n",
    "# Create DataFrame from execution times\n",
    "df_execution_times = pd.DataFrame(execution_times)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_execution_times.to_csv(f\"execution_times_{methodname}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
